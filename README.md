ğŸ§  localgpt-llama.rn
A privacy-focused React Native application that runs LLaMA-based language models locally on-device using llama.rn. No internet required. 100% offline. Ideal for private, secure, mobile AI experiences.

ğŸ“² Key Features
ğŸ”’ Fully local LLM inference â€” powered by llama.rn

ğŸ§  LocalGPT experience â€” lightweight, fast, and private

ğŸ“¦ Model selection UI â€” choose from multiple models

ğŸ”— Direct Hugging Face integration â€” fetch and load models easily

ğŸ“± Device hardware check â€” automatically checks for minimum requirements (RAM, CPU, etc.)

ğŸ¨ Beautiful React Native UI â€” built for both Android & iOS

ğŸš€ Getting Started
âœ… Make sure you've completed the React Native Environment Setup before proceeding.

1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/K-Ananthamoorthy/localgpt-llama.rn.git
cd localgpt-llama.rn
2. Install Dependencies
bash
Copy
Edit
# With npm
npm install

# OR with yarn
yarn install
3. Start the Metro Bundler
bash
Copy
Edit
npm start
# OR
yarn start
4. Run the App
Android
bash
Copy
Edit
npm run android
# OR
yarn android
iOS
Make sure CocoaPods are installed and synced:

bash
Copy
Edit
bundle install
bundle exec pod install
Then:

bash
Copy
Edit
npm run ios
# OR
yarn ios
ğŸ§ª Development Notes
Entry point: App.tsx

Main source code: src/ folder

Model management and hardware checks are handled via utilities in src/utils/

Model downloads use Hugging Face URLs; you can customize them in config

ğŸ“ Folder Structure
bash
Copy
Edit
localgpt-llama.rn/
â”‚
â”œâ”€â”€ android/              # Android native code
â”œâ”€â”€ ios/                  # iOS native code
â”œâ”€â”€ src/                  # Main app logic
â”‚   â”œâ”€â”€ components/       # UI components
â”‚   â”œâ”€â”€ screens/          # App screens
â”‚   â”œâ”€â”€ utils/            # Model download, hardware checks, etc.
â”‚   â””â”€â”€ config/           # Constants and model settings
â”œâ”€â”€ App.tsx               # App entry point
â”œâ”€â”€ package.json
â”œâ”€â”€ README.md             # This file
â””â”€â”€ ...
âœ… Requirements
Minimum 4GB RAM on device for most LLaMA models

Works on physical Android and iOS devices (simulators may not handle model inference)

llama.rn backend integrated directly (no llama.run)

ğŸ“š Resources
React Native Docs

llama.rn GitHub

Hugging Face Models

CocoaPods Docs

ğŸ¤ Contributing
Pull requests and issues are welcome! If you're using the app or improving it, feel free to fork and contribute.

ğŸ›¡ï¸ License
This project is licensed under the MIT License.
